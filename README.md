# Statistical-Natural-Language-Processing-Spring-2025

**Course Information**
From the course catalog:
This course introduces the key concepts underlying statistical natural language
processing. Students will learn a variety of techniques for the computational mod-
eling of natural language, including n-gram models, smoothing, Hidden Markov
models, Bayesian Inference, Expectation Maximization, Viterbi, Inside-Outside
Algorithm for Probabilistic Context-Free Grammars, and higher-order language
models.
NOTE: The main programming language used in the course will be Python.
For more information, see the course overview page.

**Course Objectives**
In this course, we will . . .
• cover machine learning basics and text classification algorithms, such as . . .
– naive Bayes
– logistic regression
• explore a range of important natural language processing (NLP) topics, such as . . .
– word representations (ex. embeddings)
– sequence labeling (part of speech tagging, shallow parsing/chunking, etc.)
– structured prediction (chart-based parsing, transition-based dependency parsing,
etc.)
**Learning Outcomes**
Students will be . . .
• familiar with a variety of natural language processing (NLP) tasks
• capable of comparing techniques for word and document representations
• tasked with implementing a small subset of the algorithms/architectures covered in
this class
• apply what they have learned in a course project
